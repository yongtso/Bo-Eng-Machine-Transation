{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplistic T5 model with no fancy tricks\n",
    "\n",
    "The previous example of Chinese-English machine translation has the following problems: \n",
    "\n",
    "<ul>\n",
    "    <li>Dataset is trash</li>\n",
    "    <li>Includes too many tricks (scheduler, parameter freezing, callback, metrics) that I cannot handle</li>\n",
    "</ul>\n",
    "\n",
    "Now write a T5 Chinese-English translator with better data and no fancy trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "from transformers import (\n",
    "    T5Config, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The entire data is too large to load directly into memory. For now, only load the first `nLine` lines.  \n",
    "\n",
    "Learn to handle big data with PyTorch dataloader if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 248 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我们赞扬和鼓励国际组织--包括联合国各专门机构的努力,它们为满足这些需要和愿望而工作。</td>\n",
       "      <td>We applaud and encourage the efforts of intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这些课程的开设除其他之外，主要利用本地区大学研究所、瑞典航天公司卫星图像公司和瑞典土地调查局...</td>\n",
       "      <td>The courses are based on the resources of, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>在秘鲁，只要他们行为端正和胜任本职工作，法官的任职期安全和服务的稳定性就得到保障。</td>\n",
       "      <td>In Peru, judges were guaranteed security of te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. 在巴拿马，体现敌对行动或武装冲突中儿童权利的规范与对待国际人道主义法规范的态度相符，以...</td>\n",
       "      <td>4. In Panama the norms enshrining the rights o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50. 广播、函授教育当然不能与教师引导的交互式学习对话相提并论，但由于至少有50%的非洲儿...</td>\n",
       "      <td>50. Distance learning was certainly not equiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>12.3 最初保健护理组对调查人口提供的妇女保健服务 43</td>\n",
       "      <td>Coverage of the census population by primary h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>一些非政府组织已经检查和批评过这方面的条件,并已将此记录在案。</td>\n",
       "      <td>Those conditions have been examined and critic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>·废物管理(包括污水处理)</td>\n",
       "      <td>● Waste management (including sewage treatment)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>这一过程将分阶段进行,最多由9个身份查验中心同时操作。</td>\n",
       "      <td>The process will be conducted in successive ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>一些代表团强调在进行任何进一步的裁员时，有必要考虑到工作人员福利。</td>\n",
       "      <td>A number of delegations emphasized the importa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      zh  \\\n",
       "0            我们赞扬和鼓励国际组织--包括联合国各专门机构的努力,它们为满足这些需要和愿望而工作。   \n",
       "1      这些课程的开设除其他之外，主要利用本地区大学研究所、瑞典航天公司卫星图像公司和瑞典土地调查局...   \n",
       "2              在秘鲁，只要他们行为端正和胜任本职工作，法官的任职期安全和服务的稳定性就得到保障。   \n",
       "3      4. 在巴拿马，体现敌对行动或武装冲突中儿童权利的规范与对待国际人道主义法规范的态度相符，以...   \n",
       "4      50. 广播、函授教育当然不能与教师引导的交互式学习对话相提并论，但由于至少有50%的非洲儿...   \n",
       "...                                                  ...   \n",
       "99995                      12.3 最初保健护理组对调查人口提供的妇女保健服务 43   \n",
       "99996                    一些非政府组织已经检查和批评过这方面的条件,并已将此记录在案。   \n",
       "99997                                      ·废物管理(包括污水处理)   \n",
       "99998                        这一过程将分阶段进行,最多由9个身份查验中心同时操作。   \n",
       "99999                  一些代表团强调在进行任何进一步的裁员时，有必要考虑到工作人员福利。   \n",
       "\n",
       "                                                      en  \n",
       "0      We applaud and encourage the efforts of intern...  \n",
       "1      The courses are based on the resources of, int...  \n",
       "2      In Peru, judges were guaranteed security of te...  \n",
       "3      4. In Panama the norms enshrining the rights o...  \n",
       "4      50. Distance learning was certainly not equiva...  \n",
       "...                                                  ...  \n",
       "99995  Coverage of the census population by primary h...  \n",
       "99996  Those conditions have been examined and critic...  \n",
       "99997    ● Waste management (including sewage treatment)  \n",
       "99998  The process will be conducted in successive ph...  \n",
       "99999  A number of delegations emphasized the importa...  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "enFile = open('./en-zh/UNv1.0.en-zh.en', 'r', encoding = 'utf-8')\n",
    "zhFile = open('./en-zh/UNv1.0.en-zh.zh', 'r', encoding = 'utf-8')\n",
    "\n",
    "nSkip = 0\n",
    "nLine = 100000\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "for i in range(nSkip): \n",
    "    zhFile.readline()\n",
    "    enFile.readline()\n",
    "\n",
    "for i in range(nLine): \n",
    "    zhLine = zhFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    dataMatrix.append([zhLine, enLine])\n",
    "    \n",
    "df_UN = pd.DataFrame(dataMatrix, columns = ['zh', 'en']).sample(frac=1).reset_index(drop=True) # Shuffle the data\n",
    "df_UN\n",
    "\n",
    "# Notice: The run time of appending rows in DataFrame is notoriously long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and PyTorch `Dataset`\n",
    "\n",
    "We first instantiate SentencePiece tokenizers and train them on our data. \n",
    "\n",
    "<b style=\"color:red;\">Warning!</b> For some reason I can no longer find the API for `SentencePieceBPETokenizer`. Did huggingface deprecate the old version tokenizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to store all texts in file before training tokenizer\n",
    "pathAllZh = './en-zh/allZh.txt'\n",
    "pathAllEn = './en-zh/allEn.txt'\n",
    "\n",
    "zhTextsUN = df_UN['zh'].tolist()\n",
    "enTextsUN = df_UN['en'].tolist()\n",
    "\n",
    "with open(pathAllZh, 'w', encoding = 'utf-8') as file:\n",
    "    for line in zhTextsUN:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTextsUN:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese tokenizer vocab size: 32128\n",
      "English tokenizer vocab size: 32128\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train tokenizers \n",
    "# Warning T5 tokenizer has default vocab size 32128. We should make sure the vocab size of tokenizers and T5 model match. \n",
    "\n",
    "zhTokenizer = SentencePieceBPETokenizer()\n",
    "zhTokenizer.train([pathAllZh], vocab_size = 32128, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([pathAllEn], vocab_size = 32128, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "print('Chinese tokenizer vocab size:', zhTokenizer.get_vocab_size())\n",
    "print('English tokenizer vocab size:', enTokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about tokenizer, see `Bo-Eng-Machine-Transation/warm_up_Chinese_English/01_practice_ch_en_tranlation.ipynb`. \n",
    "\n",
    "Now define PyTorch `DataLoader`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, zhTexts, enTexts, zhTokenizer, enTokenizer, zhMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.zhTexts = zhTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.zhTokenizer = zhTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.zhTokenizer.enable_padding(length = zhMaxLen)\n",
    "        self.zhTokenizer.enable_truncation(max_length = zhMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.zhTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        zhOutputs = self.zhTokenizer.encode(self.zhTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        zhEncoding = zhOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        zhMask = zhOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(zhEncoding), \n",
    "            'source_mask': torch.tensor(zhMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "Use Pytorch-lighning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "     \n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True,     # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        \n",
    "        self.zhTokenizer = hparams['zhTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        # I have no idea why to configure parameter this way \n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # parameter with weight decay \n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' not in name and 'LayerNorm.weight' not in name)], \n",
    "                'weight_decay': self.hparams['weight_decay'], \n",
    "            }, \n",
    "            {\n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' in name or 'LayerNorm.weight' in name)], \n",
    "                'weight_decay': 0.0, \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids'] \n",
    "        labels[labels[:, ] == 0] = -100    # Change the pad id from 0 to -100, but I do not know why the example chooses to do so. I will comment it out for now\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            labels = labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            zhTexts = zhTextsUN[start_idx:end_idx], \n",
    "            enTexts = enTextsUN[start_idx:end_idx], \n",
    "            zhTokenizer = self.hparams['zhTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            zhMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(zhTextsUN))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "        \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(zhTextsUN))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(zhTextsUN))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(zhTextsUN))\n",
    "        end_idx = len(zhTextsUN)\n",
    "        return self._get_dataloader(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'zhTokenizer': zhTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.85, \n",
    "    'val_percentage': 0.13, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'num_gpu': 0, \n",
    "    'weight_decay': 0, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('t5simple_' + now.strftime(\"%Y-%d-%m-%Y--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='__01_t5simple_2020-06-12-2020--19=55=07.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Text: 我们赞和鼓励国际组织--包括联合国各专门机构的努力,它们为满足这些需要和愿望而工作。\n",
      "\n",
      "Actual translation: We applaud and encourage the efforts of international organizations — including United Nations specialized agencies, which work to satisfy these needs and aspirations.\n",
      "\n",
      "Predicted translation: We are also being made to the United Nations and the United Nations system for the United Nations system for the United Nations system for the United Nations system and the United Nations system for the United Nations system for the United Nations system for the United Nations system and the United Nations system for the United Nations system. It is also a number of human rights in the field of human rights and fundamental freedoms. The United Nations. It is also a number of human rights in the field of human rights and fundamental freedoms\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 这些课程的开设除其他之外,主要利用本地区大学研究所、瑞典航天公司卫星图公司和瑞典土地调查局的资源。\n",
      "\n",
      "Actual translation: The courses are based on the resources of, inter alia, university institutes, the SSC Satellitbild company and the Swedish Land Survey in the area.\n",
      "\n",
      "Predicted translation: The Committee recommends that the State party has been made to be taken to be a number of the State party to be held in the case of the State party. The Committee on the Rights of the Child in the field of human rights in the field of human rights in the field of human rights in the field of human rights in the field of human rights in the field of human rights in the field of human rights and fundamental freedoms. The Committee on the Rights of the Child. The Committee on the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 在秘鲁,只要他们行为正和任本职工作,法官的任职期安全和服务的稳定性就得到保障。\n",
      "\n",
      "Actual translation: In Peru, judges were guaranteed security of tenure and permanence of service as long as they showed appropriate conduct and fitness for their functions.\n",
      "\n",
      "Predicted translation: The Committee recommends that the State party has not been made to the State party to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken into account. The Committee on the Rights of the Child. The Committee on the Rights of the Child in the International Covenant on Civil and Political Rights. The Committee on\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 4. 在巴拿马,体现对行动或武装冲突中儿童权利的规范与对待国际人道主义法规范的态度相符,以法律术语拟定了在这个特定情况下适用于保护儿童和青少年人权的一系列义务。\n",
      "\n",
      "Actual translation: 4. In Panama the norms enshrining the rights of the child in hostilities or armed conflicts are consistent with the treatment of the norms of international humanitarian law, formulated in legal terms in a series of obligations which as a result afford protection of human rights, in this particular case applying to children and adolescents.\n",
      "\n",
      "Predicted translation: 4. The Committee recommends that the State party has not been made to the State party to be held in the State party to be held in the State party to be held in the State party to be held in the State party to be held in the State party. The Committee that the State party has been made to be held in the State party. The Committee that the State party has been made to be held in the State party. The Committee that the State party has been made to be held in\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 50. 广播、授教育当然不能与教师引导的交互式学习对话相提并论,但由于至少有50%的非洲儿童未入学,这明显是一种改善。\n",
      "\n",
      "Actual translation: 50. Distance learning was certainly not equivalent to a teacher-led interactive learning dialogue, but it was clearly an improvement on a situation where at least 50 per cent of African children were not in classrooms.\n",
      "\n",
      "Predicted translation: The Committee recommends that the State party has been made to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken to be taken into account the State party. The Committee that the State party. The Committee that the State party has been made to the State party. The Committee that the State party. The Committee had been made to be\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 比利时\n",
      "\n",
      "Actual translation: Belgium\n",
      "\n",
      "Predicted translation: ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.ts.\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 联 合 国\n",
      "\n",
      "Actual translation: UNITED NATIONS\n",
      "\n",
      "Predicted translation: UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED NATIONS UNITED\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 参与联合国犯罪势和刑事司法系统运作情况调查以及国际犯罪(受害者)调查可改进国家一级数据收集和分析的程序。\n",
      "\n",
      "Actual translation: Participation in the United Nations surveys of crime trends and operations of criminal justice systems and in the international crime (victim) surveys has the potential to improve procedures for the collection and analysis of data at the national level.\n",
      "\n",
      "Predicted translation: The Committee recommends that the State party has not been made to the State party to be taken to be taken to be taken into account in the field of human rights. The Committee on the Rights of the Child. The Committee on the Rights of the Child in the field of human rights in the field of human rights in the field of human rights in the field of human rights. The Committee on the Rights of the Child. The Committee on the Rights of the Child on the Rights of the Child on\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testting without help of `pl.LightningModule`\n",
    "# start_idx = int((hparams['train_percentage'] + hparams['val_percentage']) * len(zhTextsUN))\n",
    "# end_idx = len(zhTextsUN)\n",
    "start_idx = 0\n",
    "end_idx = 8\n",
    "\n",
    "testset = MyDataset(\n",
    "    zhTexts = zhTextsUN[start_idx:end_idx], \n",
    "    enTexts = enTextsUN[start_idx:end_idx], \n",
    "    zhTokenizer = hparams['zhTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    zhMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'], \n",
    "    max_length = hparams['max_output_len'], \n",
    "#     num_beams = 2, \n",
    "#     repetition_penalty = 2.5, \n",
    "#     length_penalty = 1.0, \n",
    "#     early_stopping = True\n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [zhTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Chinese Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T5FineTuner(hparams).parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
