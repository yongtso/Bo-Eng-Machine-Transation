{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplistic T5 model with no fancy tricks\n",
    "\n",
    "The previous example of Chinese-English machine translation has the following problems: \n",
    "\n",
    "<ul>\n",
    "    <li>Dataset is trash</li>\n",
    "    <li>Includes too many tricks (scheduler, parameter freezing, callback, metrics) that I cannot handle</li>\n",
    "</ul>\n",
    "\n",
    "Now write a T5 Chinese-English translator with better data and no fancy trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The entire data is too large to load directly into memory. For now, only load the first `nLine` lines.  \n",
    "\n",
    "Learn to handle big data with PyTorch dataloader if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 243 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>第918(1994)号决议</td>\n",
       "      <td>RESOLUTION 918 (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994年5月17日安全理事会第3377次会议通过</td>\n",
       "      <td>Adopted by the Security Council at its 3377th ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>安全理事会，</td>\n",
       "      <td>The Security Council,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>重申其以往关于卢旺达局势的所有决议，特别是成立联合国卢旺达援助团(联卢援助团)的1993年1...</td>\n",
       "      <td>Reaffirming all its previous resolutions on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>回顾安理会主席以安理会名义在1994年4月7日发表的声明(S/PRST/ 1994/16)和...</td>\n",
       "      <td>Recalling the statements made by the President...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>135. 关于粮食首脑会议，应高度重视有关会议方针和执行会议决议的问题。</td>\n",
       "      <td>135. The World Food Summit should give careful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>首脑会议所通过的行动计划，应在为执行联合国其他重要会议和首脑会议的一整套方针而建立的机构中进...</td>\n",
       "      <td>The plan of action to be adopted at the Summit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>发言人还特别强调指出，各感兴趣的组织应共同合作，有效支持各国实现世界粮食安全的倡议。</td>\n",
       "      <td>Improved coordination and cooperation among al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>这并不妨碍联合国粮农组织在执行首脑会议决议中的领导作用。</td>\n",
       "      <td>That did not prevent FAO from playing a leadin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>加拿大还要求建立国家和国际机构来确保感兴趣的各方，包括非政府组织和私营部门有效参与执行上述方针。</td>\n",
       "      <td>Canada also encouraged the development of nati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      zh  \\\n",
       "0                                          第918(1994)号决议   \n",
       "1                              1994年5月17日安全理事会第3377次会议通过   \n",
       "2                                                 安全理事会，   \n",
       "3      重申其以往关于卢旺达局势的所有决议，特别是成立联合国卢旺达援助团(联卢援助团)的1993年1...   \n",
       "4      回顾安理会主席以安理会名义在1994年4月7日发表的声明(S/PRST/ 1994/16)和...   \n",
       "...                                                  ...   \n",
       "99995               135. 关于粮食首脑会议，应高度重视有关会议方针和执行会议决议的问题。   \n",
       "99996  首脑会议所通过的行动计划，应在为执行联合国其他重要会议和首脑会议的一整套方针而建立的机构中进...   \n",
       "99997         发言人还特别强调指出，各感兴趣的组织应共同合作，有效支持各国实现世界粮食安全的倡议。   \n",
       "99998                       这并不妨碍联合国粮农组织在执行首脑会议决议中的领导作用。   \n",
       "99999   加拿大还要求建立国家和国际机构来确保感兴趣的各方，包括非政府组织和私营部门有效参与执行上述方针。   \n",
       "\n",
       "                                                      en  \n",
       "0                                  RESOLUTION 918 (1994)  \n",
       "1      Adopted by the Security Council at its 3377th ...  \n",
       "2                                  The Security Council,  \n",
       "3      Reaffirming all its previous resolutions on th...  \n",
       "4      Recalling the statements made by the President...  \n",
       "...                                                  ...  \n",
       "99995  135. The World Food Summit should give careful...  \n",
       "99996  The plan of action to be adopted at the Summit...  \n",
       "99997  Improved coordination and cooperation among al...  \n",
       "99998  That did not prevent FAO from playing a leadin...  \n",
       "99999  Canada also encouraged the development of nati...  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "enFile = open('./en-zh/UNv1.0.en-zh.en', 'r', encoding = 'utf-8')\n",
    "zhFile = open('./en-zh/UNv1.0.en-zh.zh', 'r', encoding = 'utf-8')\n",
    "\n",
    "nLine = 100000\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "for i in range(nLine): \n",
    "    zhLine = zhFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    dataMatrix.append([zhLine, enLine])\n",
    "    \n",
    "df_UN = pd.DataFrame(dataMatrix, columns = ['zh', 'en'])\n",
    "df_UN\n",
    "\n",
    "# Notice: The run time of appending rows in DataFrame is notoriously long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and PyTorch `Dataset`\n",
    "\n",
    "We first instantiate SentencePiece tokenizers and train them on our data. \n",
    "\n",
    "<b style=\"color:red;\">Warning!</b> For some reason I can no longer find the API for `SentencePieceBPETokenizer`. Did huggingface deprecate the old version tokenizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to store all texts in file before training tokenizer\n",
    "pathAllZh = './en-zh/allZh.txt'\n",
    "pathAllEn = './en-zh/allEn.txt'\n",
    "\n",
    "with open(pathAllZh, 'w', encoding = 'utf-8') as file:\n",
    "    for line in df_UN['zh']:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in df_UN['en']:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train tokenizers \n",
    "zhTokenizer = SentencePieceBPETokenizer()\n",
    "zhTokenizer.train([pathAllZh], vocab_size = 500000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([pathAllEn], vocab_size = 500000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about tokenizer, see `Bo-Eng-Machine-Transation/warm_up_Chinese_English/01_practice_ch_en_tranlation.ipynb`. \n",
    "\n",
    "Now define PyTorch `DataLoader`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, zhTexts, enTexts, zhTokenizer, enTokenizer, zhMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.chTexts = chTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.chTokenizer = chTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.chTokenizer.enable_padding(length = chMaxLen)\n",
    "        self.chTokenizer.enable_truncation(max_length = chMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.chTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        zhOutputs = zhTokenizer.encode(zhTexts[idx])\n",
    "        enOutputs = enTokenizer.encode(enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        zhEncoding = zhOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        zhMask = zhOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(zhEncoding), \n",
    "            'source_mask': torch.tensor(zhMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "Use Pytorch-lighning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
