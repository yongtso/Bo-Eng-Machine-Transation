{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplistic T5 model with no fancy tricks\n",
    "\n",
    "The previous example of Chinese-English machine translation has the following problems: \n",
    "\n",
    "<ul>\n",
    "    <li>Dataset is trash</li>\n",
    "    <li>Includes too many tricks (scheduler, parameter freezing, callback, metrics) that I cannot handle</li>\n",
    "</ul>\n",
    "\n",
    "Now write a T5 Chinese-English translator with better data and no fancy trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The entire data is too large to load directly into memory. For now, only load the first `nLine` lines.  \n",
    "\n",
    "Learn to handle big data with PyTorch dataloader if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10. 会议上人们反复强调，发展中国家在采用空间技术方面的问题不是技术本身的问题，因为技术是...</td>\n",
       "      <td>It was stressed repeatedly during the meeting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>冰冻面制图和监测工作经常进行，重点是斯瓦巴德周围及冰层边缘优先区域。</td>\n",
       "      <td>Ice mapping and monitoring have been performed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>该《法案》的基本目的是把毛利族人的土地保留在与有关土地相关的传统族裔后代的手中。</td>\n",
       "      <td>The general theme of the Act was the retention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>使轨道实验室准备就绪，以供使用。</td>\n",
       "      <td>● Readying the orbital laboratory for use.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A．国际减灾十年的作用和</td>\n",
       "      <td>A. The role of the International Decade for Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>现阶段值得注意的是，对未来的分析受到超出仅仅对这一问题进行数学描述的诸多因素的影响，如：</td>\n",
       "      <td>It is worth noting at this stage that the anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>然而,委员会认为关于种族歧视问题的教育可以作为关于普遍性歧视问题的教育的一部分,其中包括因其...</td>\n",
       "      <td>Nevertheless, the Committee believes that teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>(b) 1987－1989年：在军方采取打击恐怖主义活动行动的情况下恐怖主义暴力活动增加，政...</td>\n",
       "      <td>(b) 1987-1989: Terrorist violence increased in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1987年5月21日</td>\n",
       "      <td>Sixth report 21 May 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>ETS-VIII正在研制中，重点是利用在S－波段上运作的大型展开式天线技术研究移动卫星通信和...</td>\n",
       "      <td>ETS-VIII is undergoing research and developmen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     zh  \\\n",
       "0     10. 会议上人们反复强调，发展中国家在采用空间技术方面的问题不是技术本身的问题，因为技术是...   \n",
       "1                    冰冻面制图和监测工作经常进行，重点是斯瓦巴德周围及冰层边缘优先区域。   \n",
       "2              该《法案》的基本目的是把毛利族人的土地保留在与有关土地相关的传统族裔后代的手中。   \n",
       "3                                      使轨道实验室准备就绪，以供使用。   \n",
       "4                                          A．国际减灾十年的作用和   \n",
       "...                                                 ...   \n",
       "9995       现阶段值得注意的是，对未来的分析受到超出仅仅对这一问题进行数学描述的诸多因素的影响，如：   \n",
       "9996  然而,委员会认为关于种族歧视问题的教育可以作为关于普遍性歧视问题的教育的一部分,其中包括因其...   \n",
       "9997  (b) 1987－1989年：在军方采取打击恐怖主义活动行动的情况下恐怖主义暴力活动增加，政...   \n",
       "9998                                         1987年5月21日   \n",
       "9999  ETS-VIII正在研制中，重点是利用在S－波段上运作的大型展开式天线技术研究移动卫星通信和...   \n",
       "\n",
       "                                                     en  \n",
       "0     It was stressed repeatedly during the meeting ...  \n",
       "1     Ice mapping and monitoring have been performed...  \n",
       "2     The general theme of the Act was the retention...  \n",
       "3            ● Readying the orbital laboratory for use.  \n",
       "4     A. The role of the International Decade for Na...  \n",
       "...                                                 ...  \n",
       "9995  It is worth noting at this stage that the anal...  \n",
       "9996  Nevertheless, the Committee believes that teac...  \n",
       "9997  (b) 1987-1989: Terrorist violence increased in...  \n",
       "9998                           Sixth report 21 May 1987  \n",
       "9999  ETS-VIII is undergoing research and developmen...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "enFile = open('./en-zh/UNv1.0.en-zh.en', 'r', encoding = 'utf-8')\n",
    "zhFile = open('./en-zh/UNv1.0.en-zh.zh', 'r', encoding = 'utf-8')\n",
    "\n",
    "nLine = 10000\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "for i in range(nLine): \n",
    "    zhLine = zhFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    dataMatrix.append([zhLine, enLine])\n",
    "    \n",
    "df_UN = pd.DataFrame(dataMatrix, columns = ['zh', 'en']).sample(frac=1).reset_index(drop=True) # Shuffle the data\n",
    "df_UN\n",
    "\n",
    "# Notice: The run time of appending rows in DataFrame is notoriously long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and PyTorch `Dataset`\n",
    "\n",
    "We first instantiate SentencePiece tokenizers and train them on our data. \n",
    "\n",
    "<b style=\"color:red;\">Warning!</b> For some reason I can no longer find the API for `SentencePieceBPETokenizer`. Did huggingface deprecate the old version tokenizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to store all texts in file before training tokenizer\n",
    "pathAllZh = './en-zh/allZh.txt'\n",
    "pathAllEn = './en-zh/allEn.txt'\n",
    "\n",
    "zhTextsUN = df_UN['zh'].tolist()\n",
    "enTextsUN = df_UN['en'].tolist()\n",
    "\n",
    "with open(pathAllZh, 'w', encoding = 'utf-8') as file:\n",
    "    for line in zhTextsUN:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTextsUN:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train tokenizers \n",
    "zhTokenizer = SentencePieceBPETokenizer()\n",
    "zhTokenizer.train([pathAllZh], vocab_size = 500000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([pathAllEn], vocab_size = 500000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about tokenizer, see `Bo-Eng-Machine-Transation/warm_up_Chinese_English/01_practice_ch_en_tranlation.ipynb`. \n",
    "\n",
    "Now define PyTorch `DataLoader`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, zhTexts, enTexts, zhTokenizer, enTokenizer, zhMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.zhTexts = zhTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.zhTokenizer = zhTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.zhTokenizer.enable_padding(length = zhMaxLen)\n",
    "        self.zhTokenizer.enable_truncation(max_length = zhMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.zhTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        zhOutputs = self.zhTokenizer.encode(self.zhTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        zhEncoding = zhOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        zhMask = zhOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(zhEncoding), \n",
    "            'source_mask': torch.tensor(zhMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "Use Pytorch-lighning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.zhTokenizer = hparams['zhTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        optimizer = AdamW(self.parameters(), lr = self.hparams['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids'] \n",
    "        # labels[labels[:, ] == 0] = -100    # Change the pad id from 0 to -100, but I do not know why the example chooses to do so. I will comment it out for now\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            labels = labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):  \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            zhTexts = zhTextsUN[start_idx:end_idx], \n",
    "            enTexts = enTextsUN[start_idx:end_idx], \n",
    "            zhTokenizer = self.hparams['zhTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            zhMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(zhTextsUN))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "        \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(zhTextsUN))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(zhTextsUN))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(zhTextsUN))\n",
    "        end_idx = len(zhTextsUN)\n",
    "        return self._get_dataloader(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'zhTokenizer': zhTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.85, \n",
    "    'val_percentage': 0.13, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'num_gpu': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a54aa4ec48049adab3322e37e8c80c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402fa37e63954de0bfa06fbfdb1358ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69d9055379347e587f883971c9033df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac6545410ae4d8d8f322f467349c91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('t5simple_' + now.strftime(\"%Y-%d-%m-%Y--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='t5simple_2020-06-12-2020--14=17=05.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Text: 116. ETS-VI是一颗2级卫星,装有一台二元推进远地点发动机,并新增了以下装置:一台用于控制南北轨道的离子发动机;高精度态控制系统;轻质结构体;轻质太阳电;卫星运载舱\n",
      "部分有防高温和控制温度的系统,以便确保运转良好。\n",
      "\n",
      "Actual translation: ETS-VI is a satellite in the 2-tonne class satellite with a bipropellant apogee engine and the following additional features: an ion engine for controlling the north-south orbit; a high-precision attitude control system; a light structural body; a light solar battery paddle; and a system of high heat prevention and heat control in the satellite bus part to ensure an excellent performance.\n",
      "\n",
      "Predicted translation: The Committee of the State party in the use of the% with the, to be a that space. was and the Space for are on its by the from of the+ as is the protection of the United Nations an rights or it development at the report of theate were theS been),; have hadH handicraf provided6 and theU members of the situation of the satellite of the national women. data\n",
      " In the( of the indigenous$\" C information3 per cent of the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 计额比参与比率应得的数额还高出13%。\n",
      "\n",
      "Actual translation: The accumulated sum was 13 percent higher than the level of participation suggested should be the case.\n",
      "\n",
      "Predicted translation: The Committee of the It is from a space with information to be its was that the UnitedH and the implementation of the State party in the programme forli, asan by theO of the situation. been taken on the/ development of the penal of theenNere had ethnic at the?; are6 rights of the national or Nations$ the use of the Government of the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 如果这一点作不到(许多卫星不携带记录设备),信息就会失。\n",
      "\n",
      "Actual translation: If that could not be done (many satellites did not carry on-board recorders) the information would be lost.\n",
      "\n",
      "Predicted translation: The Committee to the use of the State party in the% its. and thed of the United Nations with a6 has been information on the technical for the he of theo of the space by the training of the, theary had as was be that theR of the Space members of the development of the national is itá8 from the Convention of the support of the indigenous; data at theateHware provisions of the satellite of the), of women. or newZ have\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 她们还要求获得土地和信,并有权成立并管理自己的组织和制作电台节目。\n",
      "\n",
      "Actual translation: They also called for access to land and credit and the right to establish and manage their own organizations and to produce radio programmes.\n",
      "\n",
      "Predicted translation: In the Committee of the State party and a with information in the Republic of that rights to be space. TheO on theate Nations data was it is its by the requirements of the United he for the satellite of the implementation of the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 与利亚遥感总组织合作正在实施下述项目:\n",
      "\n",
      "Actual translation: In cooperation with the Syrian General Organization of Remote Sensing (GORS), the following projects are being pursued:\n",
      "\n",
      "Predicted translation: It is the Committee of the State of the national in the United Nations and to be a. The space with the use of the% had been that for the if as information on the\" at theph of the training of the laser handicraf was8,6 by the satellite$ were from the indigenous rights of the Government of the number of the first\n",
      "rial theate its\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 如果决定是定的,则新的配偶应共同负责。\n",
      "\n",
      "Actual translation: Should the decision be in the affirmative, the new spouses are to be jointly responsible.\n",
      "\n",
      "Predicted translation: The Committee of the State party to be a and that the, of the United Nations for the in the from with the Government of the spaceæ. It is its been on theate it was as by the\"\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 203. 数据采集规划天天由卫星操作中心进行。\n",
      "\n",
      "Actual translation: Planning for data acquisition is carried out daily by the Satellite Operations Centre.\n",
      "\n",
      "Predicted translation: The Committee of the State party to be a and in the use of theO. It was is that the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 这些现象与家庭贫困化加在一起,其后果更加严重。\n",
      "\n",
      "Actual translation: The effects are all the more serious in so far as they are combined with the impoverishment of families.\n",
      "\n",
      "Predicted translation: The Committee of the State party to be the space. In a with its and the It is information on in the implementation of that the Government for the, of the report of the6< of theO by the United Nations was from the indigenous asli are has been which\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testting without help of `pl.LightningModule`\n",
    "start_idx = int((hparams['train_percentage'] + hparams['val_percentage']) * len(zhTextsUN))\n",
    "end_idx = len(zhTextsUN)\n",
    "\n",
    "testset = MyDataset(\n",
    "    zhTexts = zhTextsUN[start_idx:end_idx], \n",
    "    enTexts = enTextsUN[start_idx:end_idx], \n",
    "    zhTokenizer = hparams['zhTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    zhMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = model.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'].cuda(), \n",
    "    max_length = hparams['max_output_len'], \n",
    "    num_beams = 2, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 1.0, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [zhTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Chinese Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
